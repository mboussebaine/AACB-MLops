{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import dataiku\n",
    "import pandas as pd, numpy as np\n",
    "from dataiku import pandasutils as pdu\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from dataikuapi.dss.ml import DSSPredictionMLTaskSettings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import optuna\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these constants by your own values\n",
    "XP_TRACKING_FOLDER_ID = \"37rtxeoO\"\n",
    "MLFLOW_EXPERIMENT_NAME = \"credit-card-transaction-fraud-mlflow-exp\"\n",
    "MLFLOW_CODE_ENV_NAME = \"py_38_scoring\"\n",
    "SAVED_MODEL_NAME = \"credit-card-transaction-fraud-classifier-mlflow\"\n",
    "EVALUATION_DATASET = \"transactions_known_test\"\n",
    "MODEL_NAME = \"catboost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read recipe inputs\n",
    "transactions_known_train = dataiku.Dataset(\"transactions_known_train\")\n",
    "train_df = transactions_known_train.get_dataframe()\n",
    "transactions_known_test = dataiku.Dataset(\"transactions_known_test\")\n",
    "test_df = transactions_known_test.get_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose features to include in the model\n",
    "columns_to_inlcude = ['authorized_flag', 'purchase_dow', 'purchase_weekend',\n",
    "                      'item_category', 'purchase_amount', 'signature_provided',\n",
    "                      'card_reward_program', 'card_latitude', 'card_longitude', 'card_fico_score', 'card_age',\n",
    "                      'merchant_subsector_description', 'merchant_latitude', 'merchant_longitude', 'purchase_amount_abs_z_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_ignore = [col for col in train_df.columns if col not in columns_to_inlcude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section will sync the MLFlow experiments with Dataiku\n",
    "mlflow_model_cc_transaction_fraud_folder = dataiku.Folder(XP_TRACKING_FOLDER_ID)\n",
    "client = dataiku.api_client()\n",
    "project = client.get_default_project()\n",
    "\n",
    "mlflow_extension = project.get_mlflow_extension()\n",
    "mlflow_handle = project.setup_mlflow(managed_folder=mlflow_model_cc_transaction_fraud_folder)\n",
    "\n",
    "mlflow.set_experiment(experiment_name=MLFLOW_EXPERIMENT_NAME)\n",
    "mlflow_experiment = mlflow.get_experiment_by_name(MLFLOW_EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the run with current timestamp\n",
    "def now_str() -> str:\n",
    "    return datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "\n",
    "run_name = f\"{MODEL_NAME}_{now_str()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catboost likes feature-type (categorical, numeric) indices\n",
    "nonint_features_indices = np.where((train_df.dtypes != np.int))[0]\n",
    "nonfloat_features_indices = np.where((train_df.dtypes != np.float))[0]\n",
    "categorical_features_indices = [value for value in nonint_features_indices if value in nonfloat_features_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Optuna objective function for hyperparameter tuning\n",
    "# MLFlow run within this function\n",
    "def objective(trial):\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "        run_id = run.info.run_id\n",
    "\n",
    "        mlflow.set_tag(\"model\", \"catboost\")\n",
    "        mlflow.set_tag(\"stage\", \"optuna hyperparam tuning\")\n",
    "        mlflow.set_tag(\"run_name\", run_name)\n",
    "\n",
    "        X = train_df.drop('authorized_flag', axis=1)\n",
    "        y = train_df['authorized_flag']\n",
    "\n",
    "        nonint_features_indices = np.where((X.dtypes != np.int))[0]\n",
    "        nonfloat_features_indices = np.where((X.dtypes != np.float))[0]\n",
    "        categorical_features_indices = [value for value in nonint_features_indices if value in nonfloat_features_indices]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        train_data = Pool(data=X_train, label=y_train, cat_features=categorical_features_indices)\n",
    "        test_data = Pool(data=X_test, label=y_test, cat_features=categorical_features_indices)\n",
    "\n",
    "        # Hyperparameters space here\n",
    "        param = {\n",
    "            \"objective\": \"Logloss\",\n",
    "            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.7, 0.9),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
    "            \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n",
    "            \"bootstrap_type\": trial.suggest_categorical(\n",
    "                \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n",
    "            ),\n",
    "            \"auto_class_weights\": trial.suggest_categorical(\"auto_class_weights\", [\"Balanced\",\"SqrtBalanced\"]),\n",
    "            \"ignored_features\": columns_to_ignore,\n",
    "            \"iterations\":10,\n",
    "            \"used_ram_limit\":\"6gb\"\n",
    "\n",
    "        }\n",
    "\n",
    "        # Use MLFlow to log chosen parameters\n",
    "        mlflow.log_params(param)\n",
    "        if param[\"bootstrap_type\"] == \"Bayesian\":\n",
    "            param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
    "        elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "            param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
    "\n",
    "        cat_cls = CatBoostClassifier(**param)\n",
    "        cat_cls.fit(train_data, eval_set = test_data, verbose=0)\n",
    "        mlflow.catboost.log_model(cat_cls, artifact_path=f\"{run_name}\")\n",
    "\n",
    "        preds = cat_cls.predict(X_test)\n",
    "        pred_labels = np.rint(preds)\n",
    "        roc_auc = round(roc_auc_score(y_test, pred_labels),4)\n",
    "        accuracy = round(accuracy_score(y_test, pred_labels),4)\n",
    "\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "        return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick off the Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=15, timeout=600)\n",
    "\n",
    "run_name_final = run_name + \"_final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters found by the study\n",
    "print(\"Best params\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one more MLFlow experiment to train the model on the full train/test sets\n",
    "# using the best parameters\n",
    "with mlflow.start_run(run_name=run_name_final) as run:\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    mlflow.set_tag(\"model\", \"catboost\")\n",
    "    mlflow.set_tag(\"stage\", \"final train\")\n",
    "    mlflow.set_tag(\"run_name\", run_name_final)\n",
    "\n",
    "    X_train = train_df.drop('authorized_flag', axis=1)\n",
    "    y_train = train_df['authorized_flag']\n",
    "\n",
    "    nonint_features_indices = np.where((X_train.dtypes != np.int))[0]\n",
    "    nonfloat_features_indices = np.where((X_train.dtypes != np.float))[0]\n",
    "    categorical_features_indices = [value for value in nonint_features_indices if value in nonfloat_features_indices]\n",
    "\n",
    "    X_test = test_df.drop('authorized_flag', axis=1)\n",
    "    y_test = test_df['authorized_flag']\n",
    "\n",
    "    train_data = Pool(data=X_train, label=y_train, cat_features=categorical_features_indices)\n",
    "    test_data = Pool(data=X_test, label=y_test, cat_features=categorical_features_indices)\n",
    "\n",
    "    mlflow.log_params(study.best_params)\n",
    "\n",
    "    cat_cls = CatBoostClassifier(**study.best_params,eval_metric='AUC',iterations=10)\n",
    "    cat_cls.fit(train_data, eval_set = test_data)\n",
    "    mlflow.catboost.log_model(cat_cls, artifact_path=f\"{run_name_final}\")\n",
    "\n",
    "    preds = cat_cls.predict(X_test)\n",
    "    pred_labels = np.rint(preds)\n",
    "    roc_auc = round(roc_auc_score(y_test, preds),4)\n",
    "    accuracy = round(accuracy_score(y_test, preds),4)\n",
    "\n",
    "    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MLFlow details of the final, best trained model\n",
    "experiment_id = mlflow_experiment.experiment_id\n",
    "experiment_results_df = mlflow.search_runs(experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_run_results_df = experiment_results_df[experiment_results_df['tags.run_name'] == run_name_final]\n",
    "best_run_id = latest_run_results_df.iloc[0]['run_id']\n",
    "model_path = f\"credit_card_transac/{best_run_id}/artifacts/{run_name_final}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Dataiku Saved Model (if doesn't exist already)\n",
    "sm_id = None\n",
    "for sm in project.list_saved_models():\n",
    "    if sm[\"name\"] != SAVED_MODEL_NAME:\n",
    "        continue\n",
    "    else:\n",
    "        sm_id = sm[\"id\"]\n",
    "        print(f\"Found Saved Model {sm['name']} with id {sm['id']}\")\n",
    "        break\n",
    "\n",
    "if sm_id:\n",
    "    sm = project.get_saved_model(sm_id)\n",
    "else:\n",
    "    sm = project.create_mlflow_pyfunc_model(name=SAVED_MODEL_NAME,\n",
    "                                            prediction_type=DSSPredictionMLTaskSettings.PredictionTypes.BINARY)\n",
    "    sm_id = sm.id\n",
    "    print(f\"Saved Model not found, created new one with id {sm_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the final trained model into the Dataiku Saved Model (Green Diamond)\n",
    "mlflow_version = sm.import_mlflow_version_from_managed_folder(version_id=run_name_final,\n",
    "                                                              managed_folder=XP_TRACKING_FOLDER_ID,\n",
    "                                                              path=model_path,\n",
    "                                                              code_env_name=MLFLOW_CODE_ENV_NAME)\n",
    "\n",
    "# Make this Saved Model version the active one\n",
    "sm.set_active_version(mlflow_version.version_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model metadata (target name, classes,...)\n",
    "mlflow_version.set_core_metadata('authorized_flag', [0, 1] , get_features_from_dataset=EVALUATION_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of this new version, to populate the performance screens of the saved model version in DSS\n",
    "mlflow_version.evaluate(EVALUATION_DATASET)"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "compute_37rtxeoO",
  "createdOn": 1677485869266,
  "creator": "mustapha.boussebaine@dataiku.com",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python in CPU-M-1-cpu-4Gb-Ram (env py_38)",
   "language": "python",
   "name": "py-dku-containerized-venv-py_38-cpu-m-1-cpu-4gb-ram"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "modifiedBy": "mustapha.boussebaine@dataiku.com",
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
